{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgx6Lss8RZLhYVZXR2WBK9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1\n",
        "\n",
        "First, make sure all needed packages are installed:\n"
      ],
      "metadata": {
        "id": "RQX0Czcu6gLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymystem3 nltk pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp6FqyZN6lkz",
        "outputId": "2be5cf9c-2bae-4e96-b7e7-7cd03e59dd55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymystem3 in /usr/local/lib/python3.8/dist-packages (0.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.8/dist-packages (0.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pymystem3) (2.25.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.8/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.8/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->pymystem3) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pymystem3) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pymystem3) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pymystem3) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Read the novel text from txt file\n",
        "\n",
        "Open the txt file and read from it."
      ],
      "metadata": {
        "id": "JUQCP9P6sf-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"./goncharov_obryv.txt\", \"r\")\n",
        "novel = f.read()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "kwwp_pb-tcbi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print first 500 characters to make sure everything is fine:"
      ],
      "metadata": {
        "id": "4tveqYz6wlFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(novel[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2RuviDLvfTI",
        "outputId": "f2708132-a9ce-48b1-fadd-5fd1eb286d81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Иван Александрович Гончаров\n",
            "\n",
            "Обрыв\n",
            "\n",
            "Роман в пяти частях.\n",
            "\n",
            "\n",
            "\n",
            "ЧАСТЬ ПЕРВАЯ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Два господина сидели в небрежно убранной квартире в Петербурге, на одной из больших улиц. Одному было около тридцати пяти, а другому около сорока пяти лет.\n",
            "\n",
            "Первый был Борис Павлович Райский, второй -- Иван Иванович Аянов.\n",
            "\n",
            "У Бориса Павловича была живая, чрезвычайно подвижная физиономия. С первого взгляда он казался моложе своих лет: большой белый лоб блистал свежестью, глаза менялись, то загорались мыслию, чувс\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Lemmatize text using Mystem and save it\n",
        "\n",
        "First, import Mystem and create an instance of Mystem class:"
      ],
      "metadata": {
        "id": "6vkOg3fDDOIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymystem3 import Mystem\n",
        "\n",
        "m = Mystem()"
      ],
      "metadata": {
        "id": "VXstzjieDZD5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatize the text, and print a short slice of the obtained lemmas list:"
      ],
      "metadata": {
        "id": "LZMwpZ_lyXyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = m.lemmatize(novel)"
      ],
      "metadata": {
        "id": "SI0P-iutG4Ie"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas[50:60]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIR6xO5GzTUS",
        "outputId": "e6a5f246-afdc-4537-f07b-c5f3fca17c60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['квартира', ' ', 'в', ' ', 'петербург', ', ', 'на', ' ', 'один', ' ']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to save this lemmatization into the new txt file. I'm not sure how exactly it should be formatted, so I will just join these normal forms of words into the new text and save it like that.\n",
        "\n",
        "Open the file for writing, write into it and close it:"
      ],
      "metadata": {
        "id": "1euU8jxBzxfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"lemmatized_text.txt\", \"w\")\n",
        "f.write(''.join(lemmas))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1Z1SfuGtRi7T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The saved text looks like this:"
      ],
      "metadata": {
        "id": "ELrycLbX1AQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(''.join(lemmas)[84:284])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ5FrcCG1DXZ",
        "outputId": "757cd4c0-849b-4c4c-afdd-482e9f689909"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "два господин сидеть в небрежно убирать квартира в петербург, на один из большой улица. один быть около тридцать пять, а другой около сорок пять год.\n",
            "\n",
            "первый быть борис павлович райский, второй -- иван\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3 Tokenizing the text with NLTK and use pymorphy to analyze words\n",
        "\n",
        "First, import NLTK.\n"
      ],
      "metadata": {
        "id": "WbJPsiHATKwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5H1LZG2TOZx",
        "outputId": "d200eec0-6667-4d4c-b21e-7f3fe494de46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the text and print a short slice of the obtained token list:"
      ],
      "metadata": {
        "id": "UzgIlgjM21Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(novel)"
      ],
      "metadata": {
        "id": "vnCXzUWFTWXu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[100:110]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7kVt75BTatF",
        "outputId": "783d0bbb-4381-4d5e-dc6e-efa5288150b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.', 'Иногда', 'же', 'смотрели', 'они', 'зрело', ',', 'устало', ',', 'скучно']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's analyze these tokens with pymorphy. Import the library and create an instance of it:"
      ],
      "metadata": {
        "id": "2arDv2Za3N7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy2 as mph\n",
        "\n",
        "morph = mph.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "MxYK5lRMcloM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I define a function `token_to_dict` that analyzes one token and returns a dictionary for it in desired format, and then create a list of such dictionaries by applying this function to each token. \n",
        "\n"
      ],
      "metadata": {
        "id": "C2HlXuvk3dyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_to_dict(token):\n",
        "  analysis = morph.parse(token)[0] # take the analysis with the highest score\n",
        "  return {\"lemma\": analysis.normal_form, \"word\": analysis.word, \"pos\": analysis.tag.POS}"
      ],
      "metadata": {
        "id": "7nmrp_w-c8Pc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_analysis = list(map(token_to_dict, tokens))"
      ],
      "metadata": {
        "id": "HtvT_yQM7l78"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is good time to get rid of all punctuation, as all such tokens have their POS value equal to `None` and can easily be filtered out. I also count the number of tokens before and after the filtering."
      ],
      "metadata": {
        "id": "CMBa3oJb6dlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens_analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byi6vRT_qWfg",
        "outputId": "bd3ba260-af6b-4b00-9ec1-28ebcbc6239c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "306236"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_analysis = list(filter(lambda tkn: tkn[\"pos\"] != None, tokens_analysis))"
      ],
      "metadata": {
        "id": "R7BfE9qNqZm7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens_analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70atrRnCqn0j",
        "outputId": "9588a2d0-c6f3-4569-8f95-8d13df686077"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "226709"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that approximately 80,000 tokens were filtered out. Here is a short slice of the obtaned array of dicts:"
      ],
      "metadata": {
        "id": "f15e1Zlq7WB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_analysis[100:110]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okr_YVQLgu4f",
        "outputId": "9f12d492-ebf8-40db-8e3e-97650a9dec8c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'lemma': 'этот', 'word': 'эти', 'pos': 'ADJF'},\n",
              " {'lemma': 'неизгладимый', 'word': 'неизгладимые', 'pos': 'ADJF'},\n",
              " {'lemma': 'знак', 'word': 'знаки', 'pos': 'NOUN'},\n",
              " {'lemma': 'время', 'word': 'времени', 'pos': 'NOUN'},\n",
              " {'lemma': 'и', 'word': 'и', 'pos': 'CONJ'},\n",
              " {'lemma': 'опыт', 'word': 'опыта', 'pos': 'NOUN'},\n",
              " {'lemma': 'гладкий', 'word': 'гладкие', 'pos': 'ADJF'},\n",
              " {'lemma': 'чёрный', 'word': 'чёрные', 'pos': 'ADJF'},\n",
              " {'lemma': 'волос', 'word': 'волосы', 'pos': 'NOUN'},\n",
              " {'lemma': 'падать', 'word': 'падали', 'pos': 'VERB'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a `.jsonl` file and save this information there. I import `json` library and use `json.dumps` to turn dicts into JSON strings."
      ],
      "metadata": {
        "id": "ISOxeJdx8FpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "57ictc849VSD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"analyzed_tokens.jsonl\", \"w\", encoding=\"utf-8\")\n",
        "f.write('\\n'.join(json.dumps(tkn, ensure_ascii=False) for tkn in tokens_analysis))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "S71oxAct8GUx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4.1. Calculating the shares of each part of speech\n",
        "\n",
        "To do so, I first extact a list of all unique values of POS by using `set`. Then, for each part of speech, I calculate its share by filtering only tokens of this POS, counting them, and dividing their amount by the overall token amount."
      ],
      "metadata": {
        "id": "XIFrxg8rmEKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parts_of_speech = set(tkn[\"pos\"] for tkn in tokens_analysis)\n",
        "\n",
        "for pos in parts_of_speech:\n",
        "  share = len(list(filter(lambda tkn: tkn[\"pos\"] == pos, tokens_analysis))) / len(tokens_analysis) * 100\n",
        "  print(pos + \":\\t\" + f\"{share:.2f}\" + \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdDajwcRsFSc",
        "outputId": "1f442196-b850-4261-b65e-aeb079a1688e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADJF:\t8.85%\n",
            "PREP:\t10.09%\n",
            "VERB:\t15.38%\n",
            "INFN:\t2.79%\n",
            "ADVB:\t6.90%\n",
            "PRTS:\t0.18%\n",
            "PRED:\t0.65%\n",
            "INTJ:\t0.13%\n",
            "NPRO:\t11.71%\n",
            "COMP:\t0.40%\n",
            "NUMR:\t0.45%\n",
            "PRCL:\t6.58%\n",
            "PRTF:\t0.66%\n",
            "NOUN:\t22.02%\n",
            "GRND:\t1.21%\n",
            "CONJ:\t11.15%\n",
            "ADJS:\t0.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the most frequent categories are Nouns (22%), Verbs (15%), Pronouns (11%), Conjunctives (11%) and Prepositions (10%)."
      ],
      "metadata": {
        "id": "wk78egpLD1AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4.2. Most frequent Verbs and Adverbs\n",
        "\n",
        "To obtain the top-20 most frequent Verbs and Adverbs, I will use the `Counter` class."
      ],
      "metadata": {
        "id": "CACsEIjLE9ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "6jOEJbUfH62X"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I construct the `verbs` list, which contains all verb lemmas (with repititions).\n"
      ],
      "metadata": {
        "id": "AiLujA7YPy_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verbs = [tkn[\"lemma\"] for tkn in tokens_analysis if tkn[\"pos\"] == \"VERB\"]"
      ],
      "metadata": {
        "id": "uRWgOYQ3Pvo5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Counter` class allows to obtain two lists: `Counter(verbs).keys()` is a list of unique verb lemmas, `Counter(verbs).values()` is a list of the same size that for each unique lemma contains number of its occurencies in the `verbs` list. \n",
        "\n",
        "Then, I sort `Counter(verbs).keys()` in the descending order of corresponding values in `Counter(verbs).values()` by using `zip`. I obtain `sorted_verbs` -- a list of tuples of lemmas and number of their occurencies, sorted such that the most frequent ones are at the top."
      ],
      "metadata": {
        "id": "E8dZRxpnKqcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_verbs = sorted(zip(Counter(verbs).values(), Counter(verbs).keys()), reverse=True)"
      ],
      "metadata": {
        "id": "bdQjhxlAFPpS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the top-20 verbs:"
      ],
      "metadata": {
        "id": "i-bTTqeyN2sT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for count, lemma in sorted_verbs[:20]:\n",
        "  print(lemma + \"\\t\" + str(count) + \" occurencies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhX1BdJyMwRk",
        "outputId": "417d0afb-4f97-4b5a-fb52-b6e0fa967f9a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "быть\t1938 occurencies\n",
            "сказать\t1412 occurencies\n",
            "говорить\t866 occurencies\n",
            "хотеть\t614 occurencies\n",
            "знать\t606 occurencies\n",
            "мочь\t557 occurencies\n",
            "спросить\t427 occurencies\n",
            "любить\t415 occurencies\n",
            "стать\t388 occurencies\n",
            "видеть\t386 occurencies\n",
            "думать\t354 occurencies\n",
            "пойти\t321 occurencies\n",
            "дать\t275 occurencies\n",
            "смотреть\t258 occurencies\n",
            "сделать\t242 occurencies\n",
            "идти\t218 occurencies\n",
            "заметить\t217 occurencies\n",
            "казаться\t205 occurencies\n",
            "взять\t190 occurencies\n",
            "делать\t182 occurencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the same for the adverbs."
      ],
      "metadata": {
        "id": "NNRpjUcBKThy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adverbs = [tkn[\"lemma\"] for tkn in tokens_analysis if tkn[\"pos\"] == \"ADVB\"]\n",
        "sorted_adverbs = sorted(zip(Counter(adverbs).values(), Counter(adverbs).keys()), reverse=True)"
      ],
      "metadata": {
        "id": "OhNjidSHIA4E"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for count, lemma in sorted_adverbs[:20]:\n",
        "  print(lemma + \"\\t\" + str(count) + \" occurencies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsiJZto8JvSa",
        "outputId": "95461e0d-d6a3-4ae2-c74a-db17d12065cf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "только\t775 occurencies\n",
            "ещё\t627 occurencies\n",
            "потом\t534 occurencies\n",
            "опять\t509 occurencies\n",
            "теперь\t440 occurencies\n",
            "вдруг\t386 occurencies\n",
            "там\t379 occurencies\n",
            "ничего\t375 occurencies\n",
            "где\t317 occurencies\n",
            "почти\t252 occurencies\n",
            "тут\t237 occurencies\n",
            "зачем\t224 occurencies\n",
            "уже\t212 occurencies\n",
            "уж\t196 occurencies\n",
            "здесь\t188 occurencies\n",
            "никогда\t181 occurencies\n",
            "вон\t176 occurencies\n",
            "тогда\t170 occurencies\n",
            "иногда\t166 occurencies\n",
            "тихо\t159 occurencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5. Bigrams and Trigrams\n",
        "\n",
        "Since we already got rid of punctuation in `tokens_analysis`, we can extract the `lemma` column from there to get the list of lemmas that is cleared from punctuation."
      ],
      "metadata": {
        "id": "vvsiAl5mVdbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = [tkn[\"lemma\"] for tkn in tokens_analysis]\n",
        "lemmas[100:110]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zplzGNKLVhta",
        "outputId": "943e37d4-0ebe-4bf4-f80d-a0bb1e9d9ad5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['этот',\n",
              " 'неизгладимый',\n",
              " 'знак',\n",
              " 'время',\n",
              " 'и',\n",
              " 'опыт',\n",
              " 'гладкий',\n",
              " 'чёрный',\n",
              " 'волос',\n",
              " 'падать']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the list of bigrams using NLTK:"
      ],
      "metadata": {
        "id": "n9txt9cmWPQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = list(nltk.bigrams(lemmas))"
      ],
      "metadata": {
        "id": "SnpBKyMQWPyQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sort them exactly in the same way, as in top-20 verbs and adverbs:"
      ],
      "metadata": {
        "id": "o0YdXyccWoie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_bigrams = sorted(zip(Counter(bigrams).values(), Counter(bigrams).keys()), reverse=True)"
      ],
      "metadata": {
        "id": "2tFYDHr1WvzR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for count, bigram in sorted_bigrams[:25]:\n",
        "  print(\"{0:20} \\t {1} occurencies\".format(\" \".join(bigram), str(count)))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu4zrSBDW0L-",
        "outputId": "a0b91a2b-4c96-4613-9ea3-b351a590c102"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "я не                 \t 415 occurencies\n",
            "и не                 \t 413 occurencies\n",
            "татьяна маркович     \t 377 occurencies\n",
            "сказать она          \t 326 occurencies\n",
            "она и                \t 306 occurencies\n",
            "она не               \t 305 occurencies\n",
            "у он                 \t 295 occurencies\n",
            "сказать он           \t 278 occurencies\n",
            "он не                \t 272 occurencies\n",
            "не знать             \t 272 occurencies\n",
            "на он                \t 254 occurencies\n",
            "что я                \t 253 occurencies\n",
            "не быть              \t 253 occurencies\n",
            "у она                \t 245 occurencies\n",
            "он и                 \t 243 occurencies\n",
            "не мочь              \t 239 occurencies\n",
            "как будто            \t 225 occurencies\n",
            "что он               \t 222 occurencies\n",
            "он в                 \t 220 occurencies\n",
            "на она               \t 219 occurencies\n",
            "что вы               \t 207 occurencies\n",
            "что она              \t 204 occurencies\n",
            "глядеть на           \t 201 occurencies\n",
            "и в                  \t 186 occurencies\n",
            "у я                  \t 185 occurencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same for trigrams."
      ],
      "metadata": {
        "id": "KhXZF_mnZP_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = list(nltk.trigrams(lemmas))\n",
        "sorted_trigrams = sorted(zip(Counter(trigrams).values(), Counter(trigrams).keys()), reverse=True)\n",
        "\n",
        "for count, trigram in sorted_trigrams[:25]:\n",
        "  print(\"{0:20} \\t {1} occurencies\".format(\" \".join(trigram), str(count)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp7YsFcHXGgt",
        "outputId": "0364051c-04a3-4eea-b106-4e48d26403a0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "глядеть на он        \t 63 occurencies\n",
            "в сам дело           \t 58 occurencies\n",
            "не знать что         \t 49 occurencies\n",
            "глядеть на она       \t 47 occurencies\n",
            "я не хотеть          \t 32 occurencies\n",
            "я не знать           \t 31 occurencies\n",
            "что же вы            \t 31 occurencies\n",
            "татьяна маркович и   \t 31 occurencies\n",
            "на другой день       \t 31 occurencies\n",
            "я ничего не          \t 29 occurencies\n",
            "она за рука          \t 29 occurencies\n",
            "если б я             \t 29 occurencies\n",
            "у он в               \t 27 occurencies\n",
            "сказать она и        \t 27 occurencies\n",
            "что она не           \t 26 occurencies\n",
            "и не мочь            \t 26 occurencies\n",
            "сказать татьяна маркович \t 24 occurencies\n",
            "не глядеть на        \t 24 occurencies\n",
            "взглянуть на он      \t 24 occurencies\n",
            "в этот минута        \t 24 occurencies\n",
            "поглядеть на он      \t 23 occurencies\n",
            "что я не             \t 22 occurencies\n",
            "она не быть          \t 22 occurencies\n",
            "она и не             \t 22 occurencies\n",
            "он глядеть на        \t 22 occurencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some commentaries:** We can see that the majority of bi- and trigrams contain some pronoun (я, вы, он, она,..) as they are very frequentive.\n",
        "\n",
        "They are often combined with some preposition (на, у, в,..), which are also very frequentive, or with particle не.\n"
      ],
      "metadata": {
        "id": "soTrPyg1bAA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6. Altering morphological parameters\n",
        "\n",
        "Let's take the following 5 sentences:"
      ],
      "metadata": {
        "id": "2VzSvm77ZyWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = novel[1321:1392]\n",
        "print(sentence1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCHtmd4HaakJ",
        "outputId": "056d302a-e92a-4921-cba4-c1b2602188d1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Райский одет был в домашнее серенькое пальто, сидел с ногами на диване.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2 = novel[7326:7348]\n",
        "print(sentence2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXDnwuDqa6rR",
        "outputId": "05a685dc-9bf3-4bf0-b727-b326bf763a68"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Так грозил ему доктор.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence3 = novel[10082:10119]\n",
        "print(sentence3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGZFajfIcv6E",
        "outputId": "de6b224a-8fc8-4c06-8b68-0cddaa7139cc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "А вот с женщиной биться зиму и весну!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence4 = novel[20158:20226]\n",
        "print(sentence4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV8d6f1MdUoq",
        "outputId": "b45f7569-8633-4237-fac5-95e4957d82d5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ему дали отличную квартиру, лошадей, экипаж и тысяч двадцать дохода.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence5 = novel[21500:21712]\n",
        "print(sentence5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysqvV5HM8p6K",
        "outputId": "89a58b2d-6983-447a-dc3c-a7b7a90b9b7c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Он с наслаждением и завистью припоминал анекдоты времен революции, как один знатный повеса разбил там чашку в магазине и в ответ на упреки купца перебил и переломал еще множество вещей и заплатил за весь магазин;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function takes a string, and some token-modifying functions. It tokenizes the string, applies all of the token-modifying functions, and then puts the string back together."
      ],
      "metadata": {
        "id": "u4vUnCQ4A4m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def alter_morphology(sentence, *rules):\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "  tokens_parse = [morph.parse(tkn)[0] for tkn in tokens]\n",
        "\n",
        "  result = \"\"\n",
        "  for tkn, original_tkn in zip(tokens_parse, tokens):\n",
        "    for rule in rules:\n",
        "      tkn = rule(tkn)\n",
        "\n",
        "    if result != \"\" and not mph.shapes.is_punctuation(tkn.word): result += \" \"\n",
        "    result += mph.shapes.restore_capitalization(tkn.word, original_tkn)\n",
        "  return result"
      ],
      "metadata": {
        "id": "skIMLGXMfVdT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define some token modifying operations:"
      ],
      "metadata": {
        "id": "Am42vsW7CcBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number inversion of any word (singular to plural and plural to singular)\n",
        "def number_invert(tkn):\n",
        "  if 'sing' in tkn.tag and tkn.inflect({'plur'}) != None:\n",
        "    return tkn.inflect({'plur'})\n",
        "  if 'plur' in tkn.tag and tkn.inflect({'sing'}) != None:\n",
        "    return tkn.inflect({'sing'})\n",
        "  return tkn"
      ],
      "metadata": {
        "id": "cxOIV6nmsyLO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turning verbs into imperatives when possible:\n",
        "def to_impr(tkn):\n",
        "  if tkn.inflect({'impr'}) != None:\n",
        "    return tkn.inflect({'impr'})\n",
        "  return tkn"
      ],
      "metadata": {
        "id": "d1roal9UupvM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change perfect verbs to future tense\n",
        "def perf_to_fut(tkn):\n",
        "  if 'perf' in tkn.tag and tkn.inflect({'3per', 'futr'}) != None:\n",
        "    return tkn.inflect({'3per', 'futr'})\n",
        "  return tkn"
      ],
      "metadata": {
        "id": "U0tR2982l-3q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch nominative and accusative cases:\n",
        "def case_switch(tkn):\n",
        "  if 'accs' in tkn.tag and tkn.inflect({'nomn'}) != None:\n",
        "    return tkn.inflect({'nomn'})\n",
        "  if 'nomn' in tkn.tag and tkn.inflect({'accs'}) != None:\n",
        "    return tkn.inflect({'accs'})\n",
        "  return tkn"
      ],
      "metadata": {
        "id": "JnUVn4Iu2N9z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply this operations to our sentences:"
      ],
      "metadata": {
        "id": "qp8OIJ8-8i02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence1, alter_morphology(sentence1, number_invert), sep='\\n') # Number inversion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcrvnl5Zfl7b",
        "outputId": "25254255-20b1-48a1-d1b5-bb2216e24640"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Райский одет был в домашнее серенькое пальто, сидел с ногами на диване.\n",
            "Райские одеты были в домашние серенькие пальто, сидели с ногой на диванах.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence2, alter_morphology(sentence2, case_switch, number_invert), sep='\\n') # Case switch + Number inversion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XX6Wh9WwUac",
        "outputId": "e8ddb613-0e5e-4231-914f-811ebf972d76"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Так грозил ему доктор.\n",
            "Так грозили ему докторов.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence3, alter_morphology(sentence3, to_impr, number_invert), sep='\\n') # To Imperative + Number Inversion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17pT1kQ4wUk6",
        "outputId": "39bd5031-42f5-4be7-d091-0b9f579dab13"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "А вот с женщиной биться зиму и весну!\n",
            "А вот с женщинами бейтесь зимы и вёсны!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence4, alter_morphology(sentence4, perf_to_fut), sep='\\n') # Perfect to Future"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUcJ13vEwUpn",
        "outputId": "257ec635-64f4-4d2f-c846-89fe33c14975"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ему дали отличную квартиру, лошадей, экипаж и тысяч двадцать дохода.\n",
            "Ему дадут отличную квартиру, лошадей, экипаж и тысяч двадцать дохода.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence5, alter_morphology(sentence5, to_impr, perf_to_fut), sep='\\n') # Perfect to Future"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1A43-YVwU_h",
        "outputId": "4ebd6451-997c-49cb-a8d0-599b4efdfa79"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Он с наслаждением и завистью припоминал анекдоты времен революции, как один знатный повеса разбил там чашку в магазине и в ответ на упреки купца перебил и переломал еще множество вещей и заплатил за весь магазин;\n",
            "Он с наслаждением и завистью припоминай анекдоты времён революции, как один знатный повеса разобьёт там чашку в магазине и в ответ на упрёки купца перебьёт и переломает ещё множество вещей и заплатит за весь магазин;\n"
          ]
        }
      ]
    }
  ]
}