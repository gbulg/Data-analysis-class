{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe4S/DV2ubssLJ1f8JgZmv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gbulg/Data-analysis-class/blob/main/HW1/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1\n",
        "\n",
        "## 1. Read the novel text from txt file\n",
        "\n",
        "Open the txt file, read from it and then close it."
      ],
      "metadata": {
        "id": "JUQCP9P6sf-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"./goncharov_obryv.txt\", \"r\")\n",
        "novel = f.read()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "kwwp_pb-tcbi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print first 500 characters to make sure everything is fine:"
      ],
      "metadata": {
        "id": "4tveqYz6wlFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(novel[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2RuviDLvfTI",
        "outputId": "2682ebe8-73c1-44e0-b51e-e1cfab838124"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Иван Александрович Гончаров\n",
            "\n",
            "Обрыв\n",
            "\n",
            "Роман в пяти частях.\n",
            "\n",
            "\n",
            "\n",
            "ЧАСТЬ ПЕРВАЯ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Два господина сидели в небрежно убранной квартире в Петербурге, на одной из больших улиц. Одному было около тридцати пяти, а другому около сорока пяти лет.\n",
            "\n",
            "Первый был Борис Павлович Райский, второй -- Иван Иванович Аянов.\n",
            "\n",
            "У Бориса Павловича была живая, чрезвычайно подвижная физиономия. С первого взгляда он казался моложе своих лет: большой белый лоб блистал свежестью, глаза менялись, то загорались мыслию, чувс\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2. Lemmatize text using Mystem and save it\n",
        "\n",
        "First, import Mystem and create an instance of Mystem class:"
      ],
      "metadata": {
        "id": "6vkOg3fDDOIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymystem3 import Mystem\n",
        "\n",
        "m = Mystem()"
      ],
      "metadata": {
        "id": "VXstzjieDZD5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatize the text, and print a little slice of the obtained lemmas list:"
      ],
      "metadata": {
        "id": "LZMwpZ_lyXyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = m.lemmatize(novel)"
      ],
      "metadata": {
        "id": "SI0P-iutG4Ie"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas[50:60]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIR6xO5GzTUS",
        "outputId": "65ab58db-4cac-439f-eacd-fdae90416605"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['квартира', ' ', 'в', ' ', 'петербург', ', ', 'на', ' ', 'один', ' ']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to save this lemmatization into the new txt file. I'm not sure how exactly it should be formatted, so I will just join this normal forms of words into new text and save it like that.\n",
        "\n",
        "Open the file for writing, write into it and close it:"
      ],
      "metadata": {
        "id": "1euU8jxBzxfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"goncharov_obryv_lemmas.txt\", \"w\")\n",
        "f.write(''.join(lemmas))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "1Z1SfuGtRi7T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The saved text looks like this:"
      ],
      "metadata": {
        "id": "ELrycLbX1AQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(''.join(lemmas)[84:284])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ5FrcCG1DXZ",
        "outputId": "de600c85-0432-4f89-898a-b2f4ec2bccdd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "два господин сидеть в небрежно убирать квартира в петербург, на один из большой улица. один быть около тридцать пять, а другой около сорок пять год.\n",
            "\n",
            "первый быть борис павлович райский, второй -- иван\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3 Tokenizing the text with NLTK and use pymorphy to analyze words\n",
        "\n",
        "First, import NLTK.\n"
      ],
      "metadata": {
        "id": "WbJPsiHATKwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "x5H1LZG2TOZx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the text and print a little slice of the obtained token list:"
      ],
      "metadata": {
        "id": "UzgIlgjM21Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(novel)"
      ],
      "metadata": {
        "id": "vnCXzUWFTWXu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[100:110]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7kVt75BTatF",
        "outputId": "1a32c277-8fc0-4477-8af1-04a5257a27bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.', 'Иногда', 'же', 'смотрели', 'они', 'зрело', ',', 'устало', ',', 'скучно']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's analyze these tokens with pymorphy. Import the library and create an instance of it:"
      ],
      "metadata": {
        "id": "2arDv2Za3N7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "morph = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "MxYK5lRMcloM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I define a function `token_to_dict` that analyzes one token and returns a desired dictionary for it, and then create a list of such dictionaries by applying this function to each token. \n",
        "\n"
      ],
      "metadata": {
        "id": "C2HlXuvk3dyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_to_dict(token):\n",
        "  analysis = morph.parse(token)[0] # take the analysis with the highest score\n",
        "  return {\"lemma\": analysis.normal_form, \"word\": analysis.word, \"pos\": analysis.tag.POS}"
      ],
      "metadata": {
        "id": "7nmrp_w-c8Pc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_analysis = list(map(token_to_dict, tokens))"
      ],
      "metadata": {
        "id": "HtvT_yQM7l78"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is good time to get rid of all punctuation, as all such tokens have their POS value equal to `None` and can easily be filtered out. I also count the number of tokens before and after the filtering."
      ],
      "metadata": {
        "id": "CMBa3oJb6dlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens_analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byi6vRT_qWfg",
        "outputId": "154ad03f-933a-4564-b9ed-dc8baed14960"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "306236"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_analysis = list(filter(lambda tkn: tkn[\"pos\"] != None, tokens_analysis))"
      ],
      "metadata": {
        "id": "R7BfE9qNqZm7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens_analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70atrRnCqn0j",
        "outputId": "19b39ffa-bc1c-4c40-f87a-1bc292aa57d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "226709"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that around 70,000 tokens were filtered out. And now a short slice of the obtaned array of dicts:"
      ],
      "metadata": {
        "id": "f15e1Zlq7WB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_analysis[100:110]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okr_YVQLgu4f",
        "outputId": "850bdd14-008b-4c2a-e789-3a549c557fdc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'lemma': 'этот', 'word': 'эти', 'pos': 'ADJF'},\n",
              " {'lemma': 'неизгладимый', 'word': 'неизгладимые', 'pos': 'ADJF'},\n",
              " {'lemma': 'знак', 'word': 'знаки', 'pos': 'NOUN'},\n",
              " {'lemma': 'время', 'word': 'времени', 'pos': 'NOUN'},\n",
              " {'lemma': 'и', 'word': 'и', 'pos': 'CONJ'},\n",
              " {'lemma': 'опыт', 'word': 'опыта', 'pos': 'NOUN'},\n",
              " {'lemma': 'гладкий', 'word': 'гладкие', 'pos': 'ADJF'},\n",
              " {'lemma': 'чёрный', 'word': 'чёрные', 'pos': 'ADJF'},\n",
              " {'lemma': 'волос', 'word': 'волосы', 'pos': 'NOUN'},\n",
              " {'lemma': 'падать', 'word': 'падали', 'pos': 'VERB'}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a `.jsonl` file and save this information there. I import `json` library and use `json.dumps` to turn dicts into JSON strings."
      ],
      "metadata": {
        "id": "ISOxeJdx8FpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "57ictc849VSD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"analyzed_tokens.jsonl\", \"w\", encoding=\"utf-8\")\n",
        "f.write('\\n'.join(json.dumps(tkn, ensure_ascii=False) for tkn in tokens_analysis))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "S71oxAct8GUx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4.1. Calculating the shares of each part of speech\n",
        "\n",
        "To do so, we first extact a list of all unique values of POS by using `set`. Then, for each part of speech, we calculate it's share by filtering only tokens of this POS, counting them, and dividing their amount by the overall token amount."
      ],
      "metadata": {
        "id": "XIFrxg8rmEKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parts_of_speech = set(tkn[\"pos\"] for tkn in tokens_analysis)\n",
        "\n",
        "for pos in parts_of_speech:\n",
        "  share = len(list(filter(lambda tkn: tkn[\"pos\"] == pos, tokens_analysis))) / len(tokens_analysis) * 100\n",
        "  print(pos + \":\\t\" + f\"{share:.2f}\" + \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdDajwcRsFSc",
        "outputId": "ae2dcaed-7b12-4023-8a12-869709b8deb9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADVB:\t6.90%\n",
            "NPRO:\t11.71%\n",
            "VERB:\t15.38%\n",
            "CONJ:\t11.15%\n",
            "INFN:\t2.79%\n",
            "PRCL:\t6.58%\n",
            "PRTS:\t0.18%\n",
            "ADJF:\t8.85%\n",
            "COMP:\t0.40%\n",
            "NOUN:\t22.02%\n",
            "INTJ:\t0.13%\n",
            "PRED:\t0.65%\n",
            "PRTF:\t0.66%\n",
            "NUMR:\t0.45%\n",
            "PREP:\t10.09%\n",
            "GRND:\t1.21%\n",
            "ADJS:\t0.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the most frequent categories are Nouns (22%), Verbs (15%), Pronouns (11%), Conjunctives (11%) and Prepositions (10%)."
      ],
      "metadata": {
        "id": "wk78egpLD1AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4.2. Most frequent Verbs and Adverbs\n",
        "\n",
        "To obtain the top-20 most frequent Verbs and Adverbs, i will use the `Counter` class."
      ],
      "metadata": {
        "id": "CACsEIjLE9ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "6jOEJbUfH62X"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I construct the `verbs` list, which contains all verb lemmas (with repititions).\n",
        "\n",
        "Then the `Counter` class allows to obtain two lists: `Counter(verbs).keys()` is a list of unique verb lemmas, `Counter(verbs).values()` is a list of the same size that for each unique lemma contains number of its occurencies in the `verbs` list. \n",
        "\n",
        "At last, I sort `Counter(verbs).keys()` in the descending order of corresponding values in `Counter(verbs).values()` by using `zip`. I obtain `sorted_verbs` -- a list of tuples of lemmas and the number of their occurencies, sorted such that the most frequent ones are at the top."
      ],
      "metadata": {
        "id": "E8dZRxpnKqcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verbs = [tkn[\"lemma\"] for tkn in tokens_analysis if tkn[\"pos\"] == \"VERB\"]\n",
        "sorted_verbs = sorted(zip(Counter(verbs).values(), Counter(verbs).keys()), reverse=True)"
      ],
      "metadata": {
        "id": "bdQjhxlAFPpS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the top-20 verbs:"
      ],
      "metadata": {
        "id": "i-bTTqeyN2sT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for count, lemma in sorted_verbs[:20]:\n",
        "  print(lemma + \"\\t\" + str(count) + \" occurencies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhX1BdJyMwRk",
        "outputId": "0bf06b77-18a9-497b-c4e2-6e56605f9c3d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "быть\t1938 occurencies\n",
            "сказать\t1412 occurencies\n",
            "говорить\t866 occurencies\n",
            "хотеть\t614 occurencies\n",
            "знать\t606 occurencies\n",
            "мочь\t557 occurencies\n",
            "спросить\t427 occurencies\n",
            "любить\t415 occurencies\n",
            "стать\t388 occurencies\n",
            "видеть\t386 occurencies\n",
            "думать\t354 occurencies\n",
            "пойти\t321 occurencies\n",
            "дать\t275 occurencies\n",
            "смотреть\t258 occurencies\n",
            "сделать\t242 occurencies\n",
            "идти\t218 occurencies\n",
            "заметить\t217 occurencies\n",
            "казаться\t205 occurencies\n",
            "взять\t190 occurencies\n",
            "делать\t182 occurencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the same for the adverbs."
      ],
      "metadata": {
        "id": "NNRpjUcBKThy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adverbs = [tkn[\"lemma\"] for tkn in tokens_analysis if tkn[\"pos\"] == \"ADVB\"]\n",
        "sorted_adverbs = sorted(zip(Counter(adverbs).values(), Counter(adverbs).keys()), reverse=True)"
      ],
      "metadata": {
        "id": "OhNjidSHIA4E"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for count, lemma in sorted_adverbs[:20]:\n",
        "  print(lemma + \"\\t\" + str(count) + \" occurencies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsiJZto8JvSa",
        "outputId": "594b3150-fbdf-418e-b9f9-5b3c1cfb0c69"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "только\t775 occurencies\n",
            "ещё\t627 occurencies\n",
            "потом\t534 occurencies\n",
            "опять\t509 occurencies\n",
            "теперь\t440 occurencies\n",
            "вдруг\t386 occurencies\n",
            "там\t379 occurencies\n",
            "ничего\t375 occurencies\n",
            "где\t317 occurencies\n",
            "почти\t252 occurencies\n",
            "тут\t237 occurencies\n",
            "зачем\t224 occurencies\n",
            "уже\t212 occurencies\n",
            "уж\t196 occurencies\n",
            "здесь\t188 occurencies\n",
            "никогда\t181 occurencies\n",
            "вон\t176 occurencies\n",
            "тогда\t170 occurencies\n",
            "иногда\t166 occurencies\n",
            "тихо\t159 occurencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK\n",
        "\n",
        "5. (1 point) Find top-25 bigrams and trigrams for your text (use nltk.bigrams), use only lemmas, get rid of the punctuation. Comment shortly on the results.\n",
        "\n",
        "6. (2 points) Take 3-8 sentences from the original text and substitute some morphological information, for example, change the tense of the verbs, the number of the nouns, e.g, the original Слон подарил мартышке цветы should become Слоны подарят мартышкам цветок."
      ],
      "metadata": {
        "id": "M4iDQZuZT8AF"
      }
    }
  ]
}